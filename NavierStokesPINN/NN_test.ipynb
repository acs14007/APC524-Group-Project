{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from abc import ABC, abstractmethod\n",
    "    import sys\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_probability as tfp\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import scipy.io\n",
    "    from scipy.interpolate import griddata\n",
    "    import time\n",
    "    from itertools import product, combinations\n",
    "    from pyDOE import lhs\n",
    "except:\n",
    "    print(\"One or more packages are missing...\")\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "class NeuralNetwork(ABC):\n",
    "    @abstractmethod\n",
    "    def initialize_NN(self, layers):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def neural_net(self, X, weights, biases):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, nIter: int, learning_rate: float, save_model: bool):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X_star):\n",
    "        pass\n",
    "\n",
    "class PhysicsInformedNN:\n",
    "\n",
    "    def __init__(self, x, y, t, u, v, Re, layers):\n",
    "\n",
    "        X = np.concatenate([x, y, t], 1)\n",
    "        \n",
    "        self.lb = X.min(0)\n",
    "        self.ub = X.max(0)\n",
    "                \n",
    "        self.X = X\n",
    "        \n",
    "        self.x = X[:,0:1]\n",
    "        self.y = X[:,1:2]\n",
    "        self.t = X[:,2:3]\n",
    "        \n",
    "        self.u = u\n",
    "        self.v = v\n",
    "\n",
    "        self.Re = Re\n",
    "        \n",
    "        self.layers = layers\n",
    "\n",
    "        # Initialize the NN\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # Create a list including all training variables\n",
    "        self.train_variables = self.weights + self.biases\n",
    "        # Key point: anything updates in train_variables will be \n",
    "        #            automatically updated in the original tf.Variable\n",
    "\n",
    "        self.loss = self.loss()\n",
    "        \n",
    "        \n",
    "\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "    \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "    \n",
    "    def net_psi_p(self, x, y, t):\n",
    "        psi_p = self.neural_net(tf.concat([x,t], 1), self.weights, self.biases)\n",
    "        return psi_p\n",
    "    \n",
    "    def net_NS(self, x, y, t):\n",
    "        \n",
    "        psi_and_p = self.net_psi_p(x, y, t)\n",
    "        psi = psi_and_p[:,0:1] # psi is a latent function describing u and v\n",
    "        p = psi_and_p[:,1:2]\n",
    "\n",
    "        u = tf.gradients(psi, y)[0]\n",
    "        v = -tf.gradients(psi, x)[0] \n",
    "\n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_y = tf.gradients(u, y)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        u_yy = tf.gradients(u_y, y)[0]\n",
    "        \n",
    "        v_t = tf.gradients(v, t)[0]\n",
    "        v_x = tf.gradients(v, x)[0]\n",
    "        v_y = tf.gradients(v, y)[0]\n",
    "        v_xx = tf.gradients(v_x, x)[0]\n",
    "        v_yy = tf.gradients(v_y, y)[0]\n",
    "        \n",
    "        p_x = tf.gradients(p, x)[0]\n",
    "        p_y = tf.gradients(p, y)[0]\n",
    "\n",
    "        Re = self.Re\n",
    "\n",
    "        # Functions describing the NS equation\n",
    "\n",
    "        f_u = u_t + (u*u_x + v*u_y) + p_x - (1/Re)*(u_xx + u_yy) \n",
    "        f_v = v_t + (u*v_x + v*v_y) + p_y - (1/Re)*(v_xx + v_yy)\n",
    "\n",
    "        return u, v, p, f_u, f_v\n",
    "    \n",
    "    @tf.function\n",
    "    # Loss function for the entire PINN\n",
    "    def loss(self):\n",
    "        self.u_pred, self.v_pred, self.p_pred, self.f_u_pred, self.f_v_pred = \\\n",
    "            self.net_NS(self.x, self.y, self.t)\n",
    "        \n",
    "        loss = tf.reduce_sum(tf.square(self.u - self.u_pred)) +\\\n",
    "               tf.reduce_sum(tf.square(self.v - self.v_pred)) +\\\n",
    "               tf.reduce_sum(tf.square(self.f_u_pred)) +\\\n",
    "               tf.reduce_sum(tf.square(self.f_v_pred))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, nIter: int, learning_rate: float, save_model: bool):\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        varlist = self.weights + self.biases\n",
    "        start_time = time.time()\n",
    "\n",
    "        for it in range(nIter):\n",
    "            optimizer.minimize(self.loss, varlist)\n",
    "\n",
    "            # Print training updates\n",
    "            if it % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                loss = self.loss().numpy()\n",
    "                print('It: %d, Train Loss: %.3e, Time: %.2f' % (it, loss, elapsed)) \n",
    "                start_time = time.time()\n",
    "\n",
    "        if save_model:\n",
    "            checkpoint = tf.train.Checkpoint(model=self)\n",
    "            checkpoint.save(\"trained_model_checkpoint\")\n",
    "\n",
    "    @tf.function\n",
    "    def predict(self, x_star, y_star, t_star):\n",
    "        u_star, v_star, p_star, _, _ = self.net_NS(x_star, y_star, t_star)\n",
    "        return u_star, v_star, p_star    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and reshape data\n",
    "\n",
    "N_train = 5000\n",
    "\n",
    "layers = [3, 20, 20, 20, 20, 20, 20, 20, 20, 2]\n",
    "\n",
    "# Load Data\n",
    "data = scipy.io.loadmat(\"../Reference PINN Code (Raissi)/cylinder_nektar_wake.mat\")\n",
    "        \n",
    "U_star = data['U_star'] # N x 2 x T\n",
    "P_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# Rearrange Data \n",
    "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
    "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
    "TT = np.tile(t_star, (1,N)).T # N x T\n",
    "\n",
    "UU = U_star[:,0,:] # N x T\n",
    "VV = U_star[:,1,:] # N x T\n",
    "PP = P_star # N x T\n",
    "\n",
    "x = XX.flatten()[:,None] # NT x 1\n",
    "y = YY.flatten()[:,None] # NT x 1\n",
    "t = TT.flatten()[:,None] # NT x 1\n",
    "\n",
    "u = UU.flatten()[:,None] # NT x 1\n",
    "v = VV.flatten()[:,None] # NT x 1\n",
    "p = PP.flatten()[:,None] # NT x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data    \n",
    "idx = np.random.choice(N*T, N_train, replace=False)\n",
    "x_train = tf.cast(x[idx,:], dtype=tf.float32)\n",
    "y_train = tf.cast(y[idx,:], dtype=tf.float32)\n",
    "t_train = tf.cast(t[idx,:], dtype=tf.float32)\n",
    "u_train = tf.cast(u[idx,:], dtype=tf.float32)\n",
    "v_train = tf.cast(v[idx,:], dtype=tf.float32)\n",
    "\n",
    "# x_train = x[idx,:]\n",
    "# y_train = y[idx,:]\n",
    "# t_train = t[idx,:]\n",
    "# u_train = u[idx,:]\n",
    "# v_train = v[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    File \"/var/folders/_k/0pqtbrk542z91jlm9p14ltgh0000gn/T/ipykernel_4289/2673391268.py\", line 146, in loss  *\n        self.u_pred, self.v_pred, self.p_pred, self.f_u_pred, self.f_v_pred =             self.net_NS(self.x, self.y, self.t)\n    File \"/var/folders/_k/0pqtbrk542z91jlm9p14ltgh0000gn/T/ipykernel_4289/2716962343.py\", line 112, in net_NS  *\n        psi_and_p = self.net_psi_p(x, y, t)\n    File \"/var/folders/_k/0pqtbrk542z91jlm9p14ltgh0000gn/T/ipykernel_4289/2673391268.py\", line 107, in net_psi_p  *\n        psi_p = self.neural_net(tf.concat([x,t], 1), self.weights, self.biases)\n    File \"/var/folders/_k/0pqtbrk542z91jlm9p14ltgh0000gn/T/ipykernel_4289/2673391268.py\", line 96, in neural_net  *\n        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n\n    NotImplementedError: Cannot convert a symbolic tf.Tensor (concat:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m PhysicsInformedNN(x_train, y_train, t_train, u_train, v_train, \u001b[38;5;241m100\u001b[39m, layers)\n",
      "Cell \u001b[0;32mIn[8], line 72\u001b[0m, in \u001b[0;36mPhysicsInformedNN.__init__\u001b[0;34m(self, x, y, t, u, v, Re, layers)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Key point: anything updates in train_variables will be \u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#            automatically updated in the original tf.Variable\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf24/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/_k/0pqtbrk542z91jlm9p14ltgh0000gn/T/__autograph_generated_filefhs959l4.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mu_pred, ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mv_pred, ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mp_pred, ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mf_u_pred, ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mf_v_pred \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mnet_NS, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mx, ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39my, ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mt), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_sum, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39msquare, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mu \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mu_pred,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_sum, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39msquare, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mv \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mv_pred,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_sum, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39msquare, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mf_u_pred,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_sum, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39msquare, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mf_v_pred,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/var/folders/_k/0pqtbrk542z91jlm9p14ltgh0000gn/T/__autograph_generated_filepznplb4e.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__net_NS\u001b[0;34m(self, x, y, t)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m psi_and_p \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mnet_psi_p, (ag__\u001b[38;5;241m.\u001b[39mld(x), ag__\u001b[38;5;241m.\u001b[39mld(y), ag__\u001b[38;5;241m.\u001b[39mld(t)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m psi \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(psi_and_p)[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     12\u001b[0m p \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(psi_and_p)[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/var/folders/_k/0pqtbrk542z91jlm9p14ltgh0000gn/T/__autograph_generated_filey_rv_f17.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__net_psi_p\u001b[0;34m(self, x, y, t)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m psi_p \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mneural_net, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mconcat, ([ag__\u001b[38;5;241m.\u001b[39mld(x), ag__\u001b[38;5;241m.\u001b[39mld(t)], \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mweights, ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mbiases), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/_k/0pqtbrk542z91jlm9p14ltgh0000gn/T/__autograph_generated_fileask7hcgm.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__neural_net\u001b[0;34m(self, X, weights, biases)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlen\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(weights),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 11\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mld(X) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mlb) \u001b[38;5;241m/\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mub \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mlb) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (H,)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"/var/folders/_k/0pqtbrk542z91jlm9p14ltgh0000gn/T/ipykernel_4289/2673391268.py\", line 146, in loss  *\n        self.u_pred, self.v_pred, self.p_pred, self.f_u_pred, self.f_v_pred =             self.net_NS(self.x, self.y, self.t)\n    File \"/var/folders/_k/0pqtbrk542z91jlm9p14ltgh0000gn/T/ipykernel_4289/2716962343.py\", line 112, in net_NS  *\n        psi_and_p = self.net_psi_p(x, y, t)\n    File \"/var/folders/_k/0pqtbrk542z91jlm9p14ltgh0000gn/T/ipykernel_4289/2673391268.py\", line 107, in net_psi_p  *\n        psi_p = self.neural_net(tf.concat([x,t], 1), self.weights, self.biases)\n    File \"/var/folders/_k/0pqtbrk542z91jlm9p14ltgh0000gn/T/ipykernel_4289/2673391268.py\", line 96, in neural_net  *\n        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n\n    NotImplementedError: Cannot convert a symbolic tf.Tensor (concat:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n"
     ]
    }
   ],
   "source": [
    "model = PhysicsInformedNN(x_train, y_train, t_train, u_train, v_train, 100, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(100, learning_rate=1e-3, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "from PINN import PhysicsInformedNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'NavierStokesPINN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNavierStokesPINN\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPINN\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PhysicsInformedNN\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'NavierStokesPINN'"
     ]
    }
   ],
   "source": [
    "from NavierStokesPINN.PINN import PhysicsInformedNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf24",
   "language": "python",
   "name": "tf24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
